#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGæœåŠ¡æ¨¡å—
åŸºäºç°æœ‰çš„å€’æ’ç´¢å¼•å’ŒTF-IDFå®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆ
"""

import json
import re
import os
import requests
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime

# ==================== LLM è°ƒç”¨ ====================
def call_llm_dashscope(messages, model="qwen-max"):
    """è°ƒç”¨ DashScope API (é˜¿é‡Œäº‘é€šä¹‰åƒé—®)"""
    try:
        from openai import OpenAI
        client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"DashScope APIè°ƒç”¨å¤±è´¥: {str(e)}"

def call_llm_siliconflow(messages, model="Qwen/Qwen3-8B"):
    """è°ƒç”¨ç¡…åŸºæµåŠ¨ API"""
    try:
        url = "https://api.siliconflow.cn/v1/chat/completions"
        api_key = os.getenv("SILICONFLOW_API_KEY", "")
        
        if not api_key:
            return "âŒ é”™è¯¯ï¼šæœªè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ã€‚\nè¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š\nWindows: set SILICONFLOW_API_KEY=your_api_key\nLinux/Mac: export SILICONFLOW_API_KEY=your_api_key\n\næ‚¨å¯ä»¥åœ¨ https://siliconflow.cn æ³¨å†Œå¹¶è·å–å…è´¹ API Keyã€‚"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0.3
        }
        
        response = requests.post(url, json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        if "choices" in result and len(result["choices"]) > 0:
            return result["choices"][0]["message"]["content"]
        else:
            return f"ç¡…åŸºæµåŠ¨APIè¿”å›æ ¼å¼å¼‚å¸¸: {result}"
    except requests.exceptions.HTTPError as e:
        error_msg = f"HTTPé”™è¯¯: {e.response.status_code}"
        try:
            error_detail = e.response.json()
            if "error" in error_detail:
                error_msg += f" - {error_detail['error']}"
        except:
            pass
        return f"âŒ ç¡…åŸºæµåŠ¨APIè°ƒç”¨å¤±è´¥: {error_msg}"
    except requests.exceptions.RequestException as e:
        return f"âŒ ç¡…åŸºæµåŠ¨APIè°ƒç”¨å¤±è´¥: {str(e)}"
    except Exception as e:
        return f"âŒ ç¡…åŸºæµåŠ¨APIè°ƒç”¨å¼‚å¸¸: {str(e)}"

def call_llm(messages, model="qwen-max", api_provider="dashscope"):
    """
    è°ƒç”¨ LLM (ç»Ÿä¸€æ¥å£)
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        model: æ¨¡å‹åç§°
        api_provider: APIæä¾›å•† ("dashscope" æˆ– "siliconflow")
    """
    if api_provider == "siliconflow":
        return call_llm_siliconflow(messages, model)
    else:
        return call_llm_dashscope(messages, model)

class RAGService:
    """RAGæœåŠ¡ï¼šåŸºäºå€’æ’ç´¢å¼•çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ"""
    
    def __init__(self, index_service, ollama_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ–RAGæœåŠ¡
        
        Args:
            index_service: ç´¢å¼•æœåŠ¡å®ä¾‹
            ollama_url: OllamaæœåŠ¡URL (ä¿ç•™å…¼å®¹æ€§)
        """
        self.index_service = index_service
        self.ollama_url = ollama_url
        self.default_api_provider = "siliconflow"  # é»˜è®¤ä½¿ç”¨ç¡…åŸºæµåŠ¨
        self.default_model = "Qwen/Qwen3-8B"  # é»˜è®¤ä½¿ç”¨ç¡…åŸºæµåŠ¨çš„å…è´¹æ¨¡å‹
        
    def check_ollama_connection(self) -> Tuple[bool, str]:
        """æ£€æŸ¥Ollamaè¿æ¥çŠ¶æ€ (ä¿ç•™å…¼å®¹æ€§)"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [model["name"] for model in models]
                return True, f"âœ… Ollamaè¿æ¥æˆåŠŸï¼\nå¯ç”¨æ¨¡å‹: {', '.join(model_names)}"
            else:
                return False, f"âŒ Ollamaè¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
        except requests.exceptions.RequestException as e:
            return False, f"âŒ Ollamaè¿æ¥å¤±è´¥: {str(e)}"
    
    def get_available_models(self, api_provider: str = None) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        if api_provider is None:
            api_provider = self.default_api_provider
        
        models = {
            "siliconflow": [
                "Qwen/Qwen3-8B",
                "Qwen/QwQ-32B",
                "Qwen/Qwen2.5-72B-Instruct",
                "deepseek-ai/DeepSeek-V2.5",
                "meta-llama/Llama-3.1-8B-Instruct"
            ],
            "dashscope": [
                "qwen-max",
                "qwen-plus",
                "qwen-turbo",
                "qwen2.5-72b-instruct"
            ]
        }
        return models.get(api_provider, models["siliconflow"])
    
    def retrieve_documents(self, query: str, top_k: int = 5) -> List[Tuple[str, float, str]]:
        """
        ä½¿ç”¨å€’æ’ç´¢å¼•æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆåŸºäºTF-IDFï¼‰
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›top_kä¸ªæ–‡æ¡£
            
        Returns:
            List[Tuple[str, float, str]]: (doc_id, score, content) - contentæ˜¯å®Œæ•´æ–‡æ¡£å†…å®¹
        """
        try:
            # ä½¿ç”¨ç°æœ‰çš„ç´¢å¼•æœåŠ¡è¿›è¡Œæ£€ç´¢ï¼ˆTF-IDFæ£€ç´¢ï¼‰
            results = self.index_service.search(query, top_k)
            print(f"ğŸ“– TF-IDFæ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # å°†æ‘˜è¦æ›¿æ¢ä¸ºå®Œæ•´æ–‡æ¡£å†…å®¹
            full_results = []
            for doc_id, score, summary in results:
                # è·å–å®Œæ•´æ–‡æ¡£å†…å®¹
                full_content = self.index_service.get_document(doc_id)
                if full_content:
                    full_results.append((doc_id, score, full_content))
                else:
                    # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨æ‘˜è¦
                    full_results.append((doc_id, score, summary))
            
            return full_results
        except Exception as e:
            print(f"âŒ æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def generate_answer(self, query: str, context: str, model: Optional[str] = None, api_provider: Optional[str] = None) -> str:
        """
        ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            api_provider: APIæä¾›å•† ("dashscope" æˆ– "siliconflow")
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        if model is None:
            model = self.default_model
        if api_provider is None:
            api_provider = self.default_api_provider
            
        # æ„å»ºæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"""
        
        user_prompt = f"""ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        try:
            return call_llm(messages, model, api_provider)
        except Exception as e:
            return f"âŒ è°ƒç”¨LLMå¤±è´¥: {str(e)}"
    
    def generate_answer_with_prompt(self, prompt: str, model: Optional[str] = None, api_provider: Optional[str] = None) -> str:
        """
        ç›´æ¥ä½¿ç”¨æç¤ºè¯ç”Ÿæˆå›ç­”
        
        Args:
            prompt: å®Œæ•´çš„æç¤ºè¯
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            api_provider: APIæä¾›å•† ("dashscope" æˆ– "siliconflow")
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        if model is None:
            model = self.default_model
        if api_provider is None:
            api_provider = self.default_api_provider
            
        try:
            messages = [
                {"role": "user", "content": prompt}
            ]
            return call_llm(messages, model, api_provider)
        except Exception as e:
            return f"âŒ è°ƒç”¨LLMå¤±è´¥: {str(e)}"
    
    def _react_reasoning(self, query: str, model: Optional[str], retrieval_enabled: bool, top_k: int = 5, max_steps: int = 5) -> Tuple[str, str]:
        """
        ReActé£æ ¼å¤šæ­¥æ¨ç†ï¼šThought -> Action(SEARCH/FINISH) -> Observationï¼Œå¾ªç¯ç›´åˆ°FINISHæˆ–æ­¥æ•°ä¸Šé™ã€‚
        è¿”å› (final_answer, trace_text)
        """
        if model is None:
            model = self.default_model
        
        trace_lines: List[str] = []
        observations: List[str] = []

        tool_desc = (
            "ä½ å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå·¥å…·ï¼šSEARCH(\"æŸ¥è¯¢è¯\")ï¼Œå®ƒä¼šè¿”å›ä¸æŸ¥è¯¢è¯æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨ã€‚"
        )
        format_instructions = (
            "æ¯è½®è¯·ä¸¥æ ¼è¾“å‡ºä»¥ä¸‹æ ¼å¼ä¸­çš„ä¸€è¡ŒActionï¼Œä¾¿äºè§£æï¼š\n"
            "Thought: <ä½ çš„ç®€çŸ­æ€è€ƒ>\n"
            "Action: SEARCH(\"<æŸ¥è¯¢è¯>\") æˆ– Action: FINISH(\"<æœ€ç»ˆç­”æ¡ˆ>\")\n"
            "ä¸è¦è¾“å‡ºå…¶ä»–å¤šä½™å†…å®¹ã€‚"
        )

        search_pattern = re.compile(r"Action:\s*SEARCH\(\"([\s\S]*?)\"\)")
        finish_pattern = re.compile(r"Action:\s*FINISH\(\"([\s\S]*?)\"\)")

        scratchpad = ""
        for step in range(1, max_steps + 1):
            prompt = (
                f"ä½ æ˜¯ä¸€ä¸ªä¼šé€æ­¥æ€è€ƒå¹¶åˆç†ä½¿ç”¨å·¥å…·çš„åŠ©æ‰‹ã€‚\n"
                f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\n"
                f"å·¥å…·è¯´æ˜ï¼š{tool_desc}\n"
                f"æ³¨æ„ï¼š{'å½“å‰ç¦æ­¢ä½¿ç”¨SEARCHå·¥å…·ã€‚' if not retrieval_enabled else 'å¯ä»¥ä½¿ç”¨SEARCHå·¥å…·ã€‚'}\n\n"
                f"å†å²æ¨ç†ï¼š\n{scratchpad}\n\n"
                f"è¯·å¼€å§‹ç¬¬{step}æ­¥ã€‚\n{format_instructions}"
            )
            try:
                resp = requests.post(
                    f"{self.ollama_url}/api/generate",
                    json={"model": model, "prompt": prompt, "stream": False},
                    timeout=60
                )
                if resp.status_code != 200:
                    trace_lines.append(f"ç³»ç»Ÿ: æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç  {resp.status_code}")
                    break
                text = resp.json().get("response", "").strip()
            except requests.exceptions.RequestException as e:
                trace_lines.append(f"ç³»ç»Ÿ: æ¨¡å‹è°ƒç”¨å¼‚å¸¸ {str(e)}")
                break

            # è®°å½•æ¨¡å‹è¾“å‡º
            trace_lines.append(f"Step {step} æ¨¡å‹è¾“å‡º:\n{text}")

            # è§£æåŠ¨ä½œ
            finish_match = finish_pattern.search(text)
            if finish_match:
                final_answer = finish_match.group(1)
                trace_lines.append("Action: FINISH")
                return final_answer, "\n\n".join(trace_lines)

            search_match = search_pattern.search(text)
            if search_match:
                search_query = search_match.group(1).strip()
                if retrieval_enabled:
                    # æ‰§è¡Œæ£€ç´¢
                    docs = self.retrieve_documents(search_query, top_k=top_k)
                    if not docs:
                        observation = "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
                    else:
                        # åªå–å‰3æ¡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                        obs_parts = []
                        for i, (doc_id, score, content) in enumerate(docs[:3], 1):
                            snippet = content[:400]
                            obs_parts.append(f"[{i}] id={doc_id} score={score:.4f} snippet={snippet}")
                        observation = "\n".join(obs_parts)
                    observations.append(observation)
                    trace_lines.append(f"Observation:\n{observation}")
                    scratchpad += f"Thought/Action(SEARCH): {search_query}\nObservation: {observation}\n\n"
                    continue
                else:
                    observation = "SEARCHå·¥å…·è¢«ç¦ç”¨ã€‚è¯·ç›´æ¥FINISHã€‚"
                    observations.append(observation)
                    trace_lines.append(f"Observation:\n{observation}")
                    scratchpad += f"Action(SEARCHè¢«æ‹’): {search_query}\nObservation: {observation}\n\n"
                    continue

            # è‹¥æ— æ³•è§£æåŠ¨ä½œï¼Œæç¤ºå¹¶ç»§ç»­ä¸‹ä¸€æ­¥
            notice = "æœªè§£æåˆ°æœ‰æ•ˆçš„Actionï¼Œè¯·æŒ‰æ ¼å¼è¾“å‡ºã€‚"
            trace_lines.append(f"ç³»ç»Ÿ: {notice}")
            scratchpad += f"ç³»ç»Ÿæç¤º: {notice}\n\n"

        # æœªæ˜¾å¼FINISHæ—¶ï¼Œå°è¯•è®©æ¨¡å‹åŸºäºè§‚å¯Ÿåšæœ€ç»ˆæ€»ç»“
        summary_context = "\n\n".join(observations[-3:]) if observations else ""
        final_prompt = (
            f"è¯·åŸºäºä»¥ä¸‹è§‚å¯Ÿä¸ä½ å·²æœ‰çš„æ¨ç†ï¼Œç»™å‡ºé—®é¢˜çš„æœ€ç»ˆä¸­æ–‡ç­”æ¡ˆã€‚è‹¥è§‚å¯Ÿä¸ºç©ºï¼Œè¯·ç›´æ¥æ ¹æ®å¸¸è¯†ä½œç­”ã€‚\n\n"
            f"é—®é¢˜ï¼š{query}\n\n"
            f"è§‚å¯Ÿï¼š\n{summary_context}\n\n"
            f"è¯·ç›´æ¥è¾“å‡ºç­”æ¡ˆï¼Œä¸è¦å†è¾“å‡ºæ€ç»´è¿‡ç¨‹ã€‚"
        )
        try:
            final_resp = requests.post(
                f"{self.ollama_url}/api/generate",
                json={"model": model, "prompt": final_prompt, "stream": False},
                timeout=60
            )
            if final_resp.status_code != 200:
                answer = f"âŒ å¤šæ­¥æ¨ç†æ€»ç»“å¤±è´¥ï¼ŒçŠ¶æ€ç : {final_resp.status_code}"
            else:
                answer = final_resp.json().get("response", "ç”Ÿæˆå›ç­”å¤±è´¥")
        except requests.exceptions.RequestException as e:
            answer = f"âŒ è°ƒç”¨Ollamaå¤±è´¥: {str(e)}"
        trace_lines.append("ç³»ç»Ÿ: æœªæ£€æµ‹åˆ°FINISHï¼Œå·²è¿›è¡Œè‡ªåŠ¨æ€»ç»“ã€‚")
        return answer, "\n\n".join(trace_lines)

    def rag_query(self, query: str, top_k: int = 5, model: Optional[str] = None, retrieval_enabled: bool = True, multi_step: bool = False, api_provider: Optional[str] = None) -> Dict[str, Any]:
        """
        æ‰§è¡ŒRAGæŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            model: ä½¿ç”¨çš„æ¨¡å‹
            retrieval_enabled: æ˜¯å¦å¼€å¯æ£€ç´¢å¢å¼º
            multi_step: æ˜¯å¦å¼€å¯å¤šæ­¥æ¨ç†
            api_provider: APIæä¾›å•† ("dashscope" æˆ– "siliconflow")
            
        Returns:
            Dict: åŒ…å«æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆçš„å­—å…¸
        """
        start_time = datetime.now()
        
        if api_provider is None:
            api_provider = self.default_api_provider
        if model is None:
            model = self.default_model
        
        # å¦‚æœå…³é—­æ£€ç´¢ä¸å¤šæ­¥æ¨ç†ï¼Œåˆ™ç›´æ¥é—® LLMï¼ˆæ— ä¸Šä¸‹æ–‡ç›´è¿ï¼‰
        if not retrieval_enabled and not multi_step:
            direct_prompt = f"è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\né—®é¢˜ï¼š{query}"
            answer = self.generate_answer_with_prompt(direct_prompt, model, api_provider)
            return {
                "query": query,
                "retrieved_docs": [],
                "context": "",
                "answer": answer,
                "processing_time": (datetime.now() - start_time).total_seconds(),
                "model_used": model,
                "api_provider": api_provider,
                "prompt_sent": direct_prompt
            }

        # 1) è‹¥å¼€å¯æ£€ç´¢ï¼Œå…ˆæ£€ç´¢å¹¶æ„å»ºä¸Šä¸‹æ–‡ï¼›å¦åˆ™ä¸Šä¸‹æ–‡ä¸ºç©º
        retrieved_docs = []
        context = ""
        if retrieval_enabled:
            retrieved_docs = self.retrieve_documents(query, top_k)
            # å³ä½¿æœªæ£€ç´¢åˆ°æ–‡æ¡£ï¼Œä¹Ÿç»§ç»­ï¼Œè®©æ¨¡å‹ç›´æ¥å›ç­”æˆ–å¤šæ­¥æ¨ç†
            if retrieved_docs:
                context_parts = []
                for i, (doc_id, score, content) in enumerate(retrieved_docs, 1):
                    context_parts.append(f"æ–‡æ¡£{i} (ID: {doc_id}, ç›¸å…³åº¦: {score:.4f}):\n{content}")
                context = "\n\n".join(context_parts)

        # 2) ç”Ÿæˆå›ç­”ï¼šå¤šæ­¥æ¨ç†ä¼˜å…ˆï¼Œå¦åˆ™æ™®é€šå•æ­¥å›ç­”
        if multi_step:
            answer, trace_text = self._react_reasoning(
                query=query,
                model=model,
                retrieval_enabled=retrieval_enabled,
                top_k=top_k
            )
            prompt_used = trace_text  # å°†å®Œæ•´æ¨ç†è½¨è¿¹å›æ˜¾
        else:
            # æ„å»ºæ ‡å‡†æç¤º
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”ã€‚
            
ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}
            
ç”¨æˆ·é—®é¢˜ï¼š{query}
            
è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""
            answer = self.generate_answer_with_prompt(prompt, model, api_provider)
            prompt_used = prompt
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return {
            "query": query,
            "retrieved_docs": retrieved_docs,
            "context": context,
            "answer": answer,
            "processing_time": processing_time,
            "model_used": model,
            "api_provider": api_provider,
            "prompt_sent": prompt_used if prompt_used is not None else "å¤šæ­¥æ¨ç†ï¼ˆå†…éƒ¨å¤šæç¤ºï¼‰"
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–RAGæœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        index_stats = self.index_service.get_stats()
        ollama_connected, ollama_status = self.check_ollama_connection()
        
        # æ£€æŸ¥ API Key é…ç½®
        dashscope_key = os.getenv("DASHSCOPE_API_KEY", "")
        siliconflow_key = os.getenv("SILICONFLOW_API_KEY", "")
        
        return {
            "ollama_connected": ollama_connected,
            "ollama_status": ollama_status,
            "ollama_url": self.ollama_url,
            "default_api_provider": self.default_api_provider,
            "default_model": self.default_model,
            "dashscope_configured": bool(dashscope_key),
            "siliconflow_configured": bool(siliconflow_key),
            "available_models": {
                "siliconflow": self.get_available_models("siliconflow"),
                "dashscope": self.get_available_models("dashscope")
            },
            "index_stats": index_stats
        } 